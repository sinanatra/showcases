{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcafb71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/IPython/utils/_process_posix.py:130\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     res_idx = \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mprint\u001b[39m(child.before[out_size:].decode(enc, \u001b[33m'\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m'\u001b[39m), end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/pexpect/spawnbase.py:383\u001b[39m, in \u001b[36mSpawnBase.expect_list\u001b[39m\u001b[34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/pexpect/expect.py:169\u001b[39m, in \u001b[36mExpecter.expect_loop\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m incoming = \u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.spawn.delayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/pexpect/pty_spawn.py:500\u001b[39m, in \u001b[36mspawn.read_nonblocking\u001b[39m\u001b[34m(self, size, timeout)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (timeout != \u001b[32m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m).read_nonblocking(size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/pexpect/pty_spawn.py:450\u001b[39m, in \u001b[36mspawn.read_nonblocking.<locals>.select\u001b[39m\u001b[34m(timeout)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/pexpect/utils.py:143\u001b[39m, in \u001b[36mselect_ignore_interrupts\u001b[39m\u001b[34m(iwtd, owtd, ewtd, timeout)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mollama run gemma3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/ipykernel/zmqshell.py:657\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    655\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(cmd)\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/IPython/utils/_process_posix.py:141\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    136\u001b[39m         out_size = \u001b[38;5;28mlen\u001b[39m(child.before)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/pexpect/pty_spawn.py:578\u001b[39m, in \u001b[36mspawn.sendline\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[32m    574\u001b[39m \u001b[33;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[33;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    577\u001b[39m s = \u001b[38;5;28mself\u001b[39m._coerce_send_string(s)\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinesep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/wo-es-geschah/.venv/lib/python3.13/site-packages/pexpect/pty_spawn.py:569\u001b[39m, in \u001b[36mspawn.send\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28mself\u001b[39m._log(s, \u001b[33m'\u001b[39m\u001b[33msend\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    568\u001b[39m b = \u001b[38;5;28mself\u001b[39m._encoder.encode(s, final=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchild_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "#!ollama run gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved parsed data to: output/merged_parsed_documents.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "input_dir = \"data\"\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {input_dir}\")\n",
    "\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df_temp = pd.read_csv(file)\n",
    "    df_temp[\"SourceFile\"] = os.path.basename(file)\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_text = (df['Title'].fillna('') + ' ' + df['Text'].fillna('')).str.lower()\n",
    "\n",
    "keywords = [\n",
    "    \"volksverhetzung\", \"hitlergruß\", \"hakenkreuz\",\n",
    "    \"nazi\", \"rechtsextremistisch\", \"fremdenfeindlich\",\n",
    "    \"nationalsozialismus\", \"nationalsozialistisch\",\n",
    "    \"rassismus\", \"antisemitismus\", \"homophobie\",\n",
    "    \"transphobie\", \"sieg heil\"\n",
    "]\n",
    "action_terms = [\n",
    "    \"graffiti\", \"angriff\", \"schlagen\", \"treten\", \"schubsen\",\n",
    "    \"brandanschlag\", \"beleidigung\", \"versammlung\", \"online posts\",\n",
    "    \"raubüberfall\", \"diebstahl\", \"körperverletzung\",\n",
    "    \"tötungsversuch\"\n",
    "]\n",
    "\n",
    "time_regex = re.compile(r\"\\b([0-2]?\\d[:\\.]?[0-5]?\\d)\\s*uhr\")\n",
    "date_regex = re.compile(r\"\\b(\\d{1,2}[\\./]\\d{1,2}[\\./]\\d{2,4})\\b\")\n",
    "age_regex = re.compile(r\"\\b(\\d{1,3})(?:[- ]?jährig(?:e[rn]?)?|\\sjahre alt)\\b\")\n",
    "gender_regex = re.compile(r\"\\b(mann|frau|jugendlicher|jugendliche|mädchen|junge)\\b\")\n",
    "street_regex = re.compile(r\"\\b[a-zäöüß]+(?:straße|platz|allee|ring)\\b\")\n",
    "\n",
    "diff_threshold = 0.85\n",
    "\n",
    "for col in [\n",
    "    'RightWingRelated', 'KeywordMatch', 'ExtractedDate', 'ExtractedTime',\n",
    "    'ExtractedAge', 'ExtractedGender', 'ExtractedAction', 'KeywordExtracted'\n",
    "]:\n",
    "    df[col] = None\n",
    "\n",
    "def find_keywords(text, terms, threshold):\n",
    "    tokens = set(re.findall(r\"\\w+\", text))\n",
    "    hits = []\n",
    "    for term in terms:\n",
    "        low = term.lower()\n",
    "        if re.search(rf\"\\b{re.escape(low)}\\b\", text):\n",
    "            hits.append(term)\n",
    "        else:\n",
    "            for tok in tokens:\n",
    "                if abs(len(tok) - len(low)) <= 3 and difflib.SequenceMatcher(None, tok, low).ratio() >= threshold:\n",
    "                    hits.append(term)\n",
    "                    break\n",
    "    return hits\n",
    "\n",
    "def find_keywords_with_matches(text, terms, threshold):\n",
    "    tokens = set(re.findall(r\"\\w+\", text))\n",
    "    hits = []\n",
    "    for term in terms:\n",
    "        low = term.lower()\n",
    "        for m in re.finditer(rf\"\\b{re.escape(low)}\\b\", text):\n",
    "            hits.append(m.group(0))\n",
    "        for tok in tokens:\n",
    "            if abs(len(tok) - len(low)) <= 3 and difflib.SequenceMatcher(None, tok, low).ratio() >= threshold:\n",
    "                for m in re.finditer(rf\"\\b{re.escape(tok)}\\b\", text):\n",
    "                    hits.append(m.group(0))\n",
    "    return list(set(hits)) \n",
    "\n",
    "# df['KeywordExtracted'] = None\n",
    "\n",
    "for idx, text in df_text.items():\n",
    "    kws = find_keywords(text, keywords, diff_threshold)\n",
    "    related = bool(kws)\n",
    "    df.at[idx, 'RightWingRelated'] = related\n",
    "    df.at[idx, 'KeywordMatch'] = kws\n",
    "    df.at[idx, 'KeywordExtracted'] = find_keywords_with_matches(text, keywords, diff_threshold)\n",
    "\n",
    "\n",
    "    if related:\n",
    "        date_match = date_regex.search(str(df.at[idx, 'Date']).lower())\n",
    "        df.at[idx, 'ExtractedDate'] = date_match.group(1) if date_match else df.at[idx, 'Date']\n",
    "\n",
    "        time_match = time_regex.findall(text)\n",
    "        df.at[idx, 'ExtractedTime'] = time_match\n",
    "        \n",
    "        df.at[idx, 'ExtractedAge'] = age_regex.findall(text)\n",
    "        df.at[idx, 'ExtractedGender'] = gender_regex.findall(text)\n",
    "        df.at[idx, 'ExtractedAction'] = find_keywords(text, action_terms, diff_threshold)\n",
    "\n",
    "out_path = os.path.join(output_dir, \"merged_parsed_documents.csv\")\n",
    "parsed = df[df['RightWingRelated'] == True]\n",
    "parsed.to_csv(out_path, index=False)\n",
    "print(f\"Saved parsed data to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286da67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc5b527f",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a506da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged keyword-tagged documents to: output/merged_tagged_documents.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import ollama\n",
    "\n",
    "input_dir = \"data\"\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {input_dir}\")\n",
    "\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df_temp = pd.read_csv(file)\n",
    "    df_temp[\"SourceFile\"] = os.path.basename(file)\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "keywords = [\"Volksverhetzung\", \"Hitlergruß\", \"Hakenkreuz\", \"Rechtsextremistisch\", \"Fremdenfeindlich\"]\n",
    "offence = \"Sachbeschädigung, Körperverletzung, Ehrverletzung, Äußerungsdelikt, Totschlag, Vermögensdelikt, Brandstiftung, Verbreitungsdelikt, Gefährdungsdelikt, sonstige\"\n",
    "action = \"Hakenkreuz, Hitlergruß, Graffiti, Angriff, Schlagen, Schubsen, Verwendung verfassungswidriger Symbole, Beleidigung, Äußerung, Volksverhetzung, Nazi Parole, Schmiererei, Brandanschlag, Belästigung, Bedrohung, Raubüberfall, Diebstahl, Körperverletzung, Tötungsversuch, Anspucken, Geräusch, Versammlung, Online posts, sonstige\"\n",
    "motive = \"Verleugnung des Holocaust, Rassismus, Antisemitismus, Fremdenfeindlichkeit, Homophobie, Transphobie, Anti-Islamismus, Rechtsextremismus, Nationalsozialismus, sonstige, unbestimmt\"\n",
    "\n",
    "offence_prompt = (\n",
    "    \"You are analyzing a police report that contains clear signs of hate or extremist content.\\n\"\n",
    "    \"Return a valid JSON object with the following fields:\\n\"\n",
    "    f\"- OffenceType: a list of terms from this set: {offence}\\n\"\n",
    "    f\"- Action: a list of terms from this set: {action}\\n\"\n",
    "    f\"- PossibleMotive: a list of terms from this set: {motive}\\n\\n\"\n",
    "    \"Return empty lists if no valid information is found. Respond with JSON only. No explanation or extra text.\"\n",
    ")\n",
    "\n",
    "df[\"RightWingRelated\"] = False\n",
    "df[\"KeywordMatch\"] = None\n",
    "df[\"OffenceType\"] = [[] for _ in range(len(df))]\n",
    "df[\"Action\"] = [[] for _ in range(len(df))]\n",
    "df[\"PossibleMotive\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    keyword_prompt = (\n",
    "        f\"Below is a document:\\n\\n\"\n",
    "        f\"Title: {row['Title']}\\n\\n\"\n",
    "        f\"Text: {row['Text']}\\n\\n\"\n",
    "        \"Question: Does this document mention any of the following terms: \"\n",
    "        f\"{', '.join(keywords)}?\\n\"\n",
    "        \"If yes, reply with only the term that is mentioned; if not, reply with 'None'.\"\n",
    "    )\n",
    "    keyword_hit = ollama.generate(model=\"gemma3\", prompt=keyword_prompt)[\"response\"].strip()\n",
    "    if keyword_hit in keywords:\n",
    "        df.at[i, \"RightWingRelated\"] = True\n",
    "        df.at[i, \"KeywordMatch\"] = keyword_hit\n",
    "\n",
    "        tagging_prompt = (\n",
    "            f\"Below is a document:\\n\\n\"\n",
    "            f\"Title: {row['Title']}\\n\\n\"\n",
    "            f\"Text: {row['Text']}\\n\\n\"\n",
    "            f\"{offence_prompt}\"\n",
    "        )\n",
    "        tagging_response = ollama.generate(model=\"gemma3\", prompt=tagging_prompt)[\"response\"].strip()\n",
    "        try:\n",
    "            result = json.loads(tagging_response)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r\"\\{.*\\}\", tagging_response, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    result = json.loads(match.group(0))\n",
    "                except json.JSONDecodeError:\n",
    "                    result = {\"OffenceType\": [], \"Action\": [], \"PossibleMotive\": []}\n",
    "            else:\n",
    "                result = {\"OffenceType\": [], \"Action\": [], \"PossibleMotive\": []}\n",
    "\n",
    "        df.at[i, \"OffenceType\"] = result.get(\"OffenceType\", [])\n",
    "        df.at[i, \"Action\"] = result.get(\"Action\", [])\n",
    "        df.at[i, \"PossibleMotive\"] = result.get(\"PossibleMotive\", [])\n",
    "\n",
    "output_path = os.path.join(output_dir, \"tagged_documents.csv\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved merged tagged documents to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae8f5e",
   "metadata": {},
   "source": [
    "### Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"precise-street-geo\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Extract the most specific geocodable location from the following police report.\n",
    "\n",
    "Return a single string like:\n",
    "\"Street name, Town, District\"\n",
    "Do not include quotes, explanations, or extra punctuation.\n",
    "Only return a single clean, geocodable string.\n",
    "---\n",
    "\n",
    "Text:\n",
    "{incident}\n",
    "\n",
    "Location:\n",
    "{location}\n",
    "\n",
    "Locations:\n",
    "\"\"\"\n",
    "\n",
    "right_wing_df = df[df[\"RightWingRelated\"] == True].copy()\n",
    "results = []\n",
    "\n",
    "df[\"Latitude\"] = None\n",
    "df[\"Longitude\"] = None\n",
    "\n",
    "for i, row in right_wing_df.iterrows():\n",
    "    prompt = prompt_template.format(\n",
    "        incident=row['Text'] or \"\",\n",
    "        location=row['Location'] or \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    response = get_response_from_model(prompt, model).strip()\n",
    "    response = re.sub(r\"^```(?:json|python)?\\s*\", \"\", response, flags=re.IGNORECASE)\n",
    "    response = re.sub(r\"\\s*```$\", \"\", response, flags=re.IGNORECASE)\n",
    "    response = re.sub(r\"^python\\s*\", \"\", response, flags=re.IGNORECASE)\n",
    "\n",
    "    query1 = f\"{response}, Berlin, Germany\"\n",
    "    query2 = f\"{row['Location']}, Berlin, Germany\" if pd.notna(row[\"Location\"]) else \"\"\n",
    "    query3 = \"\"\n",
    "\n",
    "    if pd.notna(row[\"Location\"]) and \",\" in row[\"Location\"]:\n",
    "        query3 = row[\"Location\"].split(\",\")[-1].strip() + \", Berlin, Germany\"\n",
    "\n",
    "    location = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        location = geocode(query1)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error geocoding primary for row {i}: {e}\")\n",
    "\n",
    "\n",
    "    if not location and query2:\n",
    "        try:\n",
    "            location = geocode(query2)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error geocoding fallback for row {i}: {e}\")\n",
    "\n",
    "\n",
    "    if not location and query3:\n",
    "        try:\n",
    "            location = geocode(query3)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error geocoding last fallback for row {i}: {e}\")\n",
    "\n",
    "\n",
    "    if location:\n",
    "        print(f\"📍 Row {i}: {location.address} → {location.latitude}, {location.longitude}\")\n",
    "        results.append({\n",
    "            \"Query\": response,\n",
    "            \"Latitude\": location.latitude,\n",
    "            \"Longitude\": location.longitude,\n",
    "            \"TextExcerpt\": row[\"Text\"],\n",
    "            \"URL\": row[\"URL\"]\n",
    "        })\n",
    "        df.at[i, \"Latitude\"] = location.latitude\n",
    "        df.at[i, \"Longitude\"] = location.longitude\n",
    "    else:\n",
    "        print(f\"⚠️ Geocoding failed for all fallbacks on row {i}: {query1} / {query2} / {query3}\")\n",
    "\n",
    "# geo_df = pd.DataFrame(results)\n",
    "# geo_df.to_csv(\"data/berlin_geocoded.csv\", index=False)\n",
    "\n",
    "df.to_csv(\"data/berlin_geocoded.csv\", index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d443b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
