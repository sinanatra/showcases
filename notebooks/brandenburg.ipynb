{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test URL: https://polizei.brandenburg.de/suche/typ/null/1/1?search%5Bquery%5D=&search%5Bregion%5D=&search%5Bdienststelle%5D=&search%5Bkategorien%5D=Kriminalit%C3%A4t&search%5BonlineDateFrom%5D=01.02.2025&search%5BdocType%5D=&search%5Btags%5D=&search%5Bzeitraum%5D=&search%5BonlineDateTo%5D=&search%5BsearchButton2%5D=Suchen+%C2%BB\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import urllib.robotparser\n",
    "import csv\n",
    "import time\n",
    "from requests.models import PreparedRequest\n",
    "\n",
    "def scrape_brandenburg_police(params):\n",
    "    base_url = \"https://polizei.brandenburg.de\"\n",
    "    url_pattern = f\"{base_url}/suche/typ/null/{{page}}/1\"\n",
    "    req = PreparedRequest()\n",
    "    req.prepare_url(url_pattern.format(page=1), params)\n",
    "    test_url = req.url\n",
    "    print(\"Test URL:\", test_url)\n",
    "    \n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(urljoin(base_url, \"robots.txt\"))\n",
    "    rp.read()\n",
    "    if not rp.can_fetch(\"*\", test_url):\n",
    "        print(f\"Scraping disallowed for URL: {test_url}\")\n",
    "        return\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    results = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        url = url_pattern.format(page=page)\n",
    "        response = session.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        ul = soup.find(\"ul\", class_=lambda x: x and \"pbb-searchlist\" in x)\n",
    "        if not ul:\n",
    "            break\n",
    "        items = ul.find_all(\"li\")\n",
    "        if not items:\n",
    "            break\n",
    "        for li in items:\n",
    "            h4 = li.find(\"h4\")\n",
    "            a = h4.find(\"a\") if h4 else None\n",
    "            if a:\n",
    "                strong = a.find(\"strong\")\n",
    "                title = strong.get_text(strip=True) if strong else a.get_text(strip=True)\n",
    "                article_url = urljoin(base_url, a.get(\"href\"))\n",
    "                art_response = session.get(article_url)\n",
    "                if art_response.status_code != 200:\n",
    "                    text = \"\"\n",
    "                    location = \"\"\n",
    "                else:\n",
    "                    art_soup = BeautifulSoup(art_response.text, 'html.parser')\n",
    "                    content = art_soup.find(\"div\", class_=\"pbb-article-text\")\n",
    "                    text = content.get_text(separator=\"\\n\", strip=True) if content else \"\"\n",
    "                    ort_tag = art_soup.find(\"p\", class_=\"pbb-ort\")\n",
    "                    landkreis_tag = art_soup.find(\"p\", class_=\"pbb-landkreis\")\n",
    "                    location = \"\"\n",
    "                    if ort_tag:\n",
    "                        location = ort_tag.get_text(strip=True)\n",
    "                    if landkreis_tag:\n",
    "                        location = f\"{location}, {landkreis_tag.get_text(strip=True)}\" if location else landkreis_tag.get_text(strip=True)\n",
    "            else:\n",
    "                title = \"\"\n",
    "                text = \"\"\n",
    "                location = \"\"\n",
    "            p = li.find(\"p\")\n",
    "            span = p.find(\"span\") if p else None\n",
    "            if span:\n",
    "                span_text = span.get_text(separator=\" \", strip=True)\n",
    "                date = span_text.split(\"Artikel vom\")[-1].strip().split()[0] if \"Artikel vom\" in span_text else \"\"\n",
    "            else:\n",
    "                date = \"\"\n",
    "            results.append({\"title\": title, \"date\": date, \"location\": location, \"text\": text})\n",
    "        page += 1\n",
    "        time.sleep(3)\n",
    "    with open('brandenburg_police_results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Title\", \"Date\", \"Location\", \"Text\"])\n",
    "        for r in results:\n",
    "            writer.writerow([r[\"title\"], r[\"date\"], r[\"location\"], r[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_params = {\n",
    "        \"search[query]\": \"\",\n",
    "        \"search[region]\": \"\",\n",
    "        \"search[dienststelle]\": \"\",\n",
    "        \"search[kategorien]\": \"Kriminalität\",\n",
    "        \"search[onlineDateFrom]\": \"01.02.2025\",\n",
    "        \"search[docType]\": \"\",\n",
    "        \"search[tags]\": \"\",\n",
    "        \"search[zeitraum]\": \"\",\n",
    "        \"search[onlineDateTo]\": \"\",\n",
    "        \"search[searchButton2]\": \"Suchen »\"\n",
    "    }\n",
    "\n",
    "scrape_brandenburg_police(query_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
