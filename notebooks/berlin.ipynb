{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polizei Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import urllib.robotparser\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def scrape_berlin_police():\n",
    "    base_url = \"https://www.berlin.de\"\n",
    "    year_url = \"https://www.berlin.de/polizei/polizeimeldungen/archiv/2025/\"\n",
    "    file_path = 'data/berlin_police_results.csv'\n",
    "    max_date = datetime.date(2025, 2, 1)\n",
    "\n",
    "    existing_urls = set()\n",
    "    existing_rows = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8', newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_urls.add(row[\"URL\"])\n",
    "                existing_rows.append(row)\n",
    "\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(urljoin(base_url, \"robots.txt\"))\n",
    "    rp.read()\n",
    "    if not rp.can_fetch(\"*\", year_url):\n",
    "        print(\"â›” Scraping not allowed by robots.txt.\")\n",
    "        return\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "    new_rows = []\n",
    "    page = 1\n",
    "    stop_scraping = False\n",
    "\n",
    "    while not stop_scraping:\n",
    "        page_url = year_url if page == 1 else f\"{year_url}?page_at_1_0={page}#headline_1_0\"\n",
    "        print(f\"ðŸ”Ž Fetching: {page_url}\")\n",
    "        try:\n",
    "            res = session.get(page_url, timeout=30)\n",
    "            if res.status_code != 200:\n",
    "                print(\"â›” Stopped. No more pages.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error fetching page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        items = soup.select(\"ul.list--tablelist > li\")\n",
    "        if not items:\n",
    "            print(\"ðŸ“­ No more list items found.\")\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            date_div = item.find(\"div\", class_=\"date\")\n",
    "            text_div = item.find(\"div\", class_=\"text\")\n",
    "            if not date_div or not text_div:\n",
    "                continue\n",
    "\n",
    "            url_rel = text_div.find(\"a\")[\"href\"]\n",
    "            article_url = urljoin(base_url, url_rel)\n",
    "            if article_url in existing_urls:\n",
    "                stop_scraping = True\n",
    "                print(\"ðŸ›‘ Reached already-saved article, stopping.\")\n",
    "                break\n",
    "\n",
    "            title = text_div.find(\"a\").get_text(strip=True)\n",
    "            date_str = date_div.get_text(strip=True).split(\" \")[0]\n",
    "            try:\n",
    "                article_date = datetime.datetime.strptime(date_str, \"%d.%m.%Y\").date()\n",
    "                if article_date < max_date:\n",
    "                    stop_scraping = True\n",
    "                    print(f\"ðŸ›‘ Reached article older than cutoff: {article_date}\")\n",
    "                    break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            location = \"\"\n",
    "            loc_span = text_div.find(\"span\", class_=\"category\")\n",
    "            if loc_span and \"Ereignisort:\" in loc_span.text:\n",
    "                location = loc_span.text.replace(\"Ereignisort:\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                art_res = session.get(article_url, timeout=30)\n",
    "                if art_res.status_code != 200:\n",
    "                    raise Exception(\"Bad status\")\n",
    "                art_soup = BeautifulSoup(art_res.text, \"html.parser\")\n",
    "                content = art_soup.find(\"div\", class_=\"textile\")\n",
    "                text = content.get_text(separator=\"\\n\", strip=True) if content else \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to fetch article: {article_url} â€“ {e}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"ðŸ“ {location} â€“ {date_str} â€“ {title}\")\n",
    "            new_rows.append({\n",
    "                \"Title\": title,\n",
    "                \"Date\": date_str,\n",
    "                \"Location\": location,\n",
    "                \"Text\": text,\n",
    "                \"URL\": article_url\n",
    "            })\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(2)\n",
    "\n",
    "    all_rows = existing_rows + new_rows\n",
    "    with open(file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"Title\", \"Date\", \"Location\", \"Text\", \"URL\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_rows)\n",
    "\n",
    "    print(f\"\\nScraping complete. {len(new_rows)} new articles saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Fetching: https://www.berlin.de/polizei/polizeimeldungen/archiv/2025/\n",
      "ðŸ“ Spandau â€“ 17.06.2025 â€“ Autofahrerin fÃ¤hrt Radfahrer an\n",
      "ðŸ›‘ Reached already-saved article, stopping.\n",
      "\n",
      "âœ… Done. 1 new articles saved to data/berlin_police_results.csv.\n"
     ]
    }
   ],
   "source": [
    "scrape_berlin_police()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
